{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nsharandroidnstudio/Anti-Israel-Sentiment-Classifier/blob/main/Clothes_Recommendation_System_Using_DenseNet121.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'fashion-product-images-dataset:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F139630%2F329006%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240713%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240713T061741Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D4ef4641bc28261ed11b197591ae3a72df39ce5f6aaad44d2c78d091491c51c0b374ae9b4de706dc93d767f64b89b51177e2a603aec0d2c1aaff37da33d565ff6ea78b244c87c8aceaa87c2f7c3ad7e9cf545be2babfadef56c0791e251a3799fc9ee2f87a9c793f23760021ae8a50e95c6e21107f7109f037cfae996c4709a6c8aef0302d2f3e11d22c3adde812e1ab352785bee4ec1b2394dd669f027bf022702e0f95e4c960c7fcb5e2c0ff7e1787fad3dae8eda64d3dcd7f2183ac76888631f911736a24abc84322eb2c9d41cf326f8e470fcb26b3ba4d8fa03558a9907c6c313c5dd0e72ebb1d00e2da90cd7fd1fb3a6c9ec93bfdcbdf5ebab141c201091'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "AIz7trebvX5U"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "5oBGMq-nvX5Y"
      },
      "cell_type": "markdown",
      "source": [
        "# Clothes Recommendation System (DenseNet121)"
      ]
    },
    {
      "metadata": {
        "id": "vRyUjGXNvX5a"
      },
      "cell_type": "markdown",
      "source": [
        "Bulding Clothes Recommendation System using DenseNet121"
      ]
    },
    {
      "metadata": {
        "id": "ozo5y8cEvX5a"
      },
      "cell_type": "markdown",
      "source": [
        "Importing libraries"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "9f4UlZG1vX5b"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from keras import Model\n",
        "from keras.applications import DenseNet121\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.densenet import preprocess_input, decode_predictions\n",
        "from keras.layers import GlobalMaxPooling2D\n",
        "from keras.utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import cv2\n",
        "import pathlib\n",
        "from sklearn.metrics.pairwise import linear_kernel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "dgfjhjy8vX5b"
      },
      "cell_type": "code",
      "source": [
        "path = '../input/fashion-product-images-dataset/fashion-dataset/fashion-dataset/'\n",
        "dataset_path = pathlib.Path(path)\n",
        "images=os.listdir(dataset_path)\n",
        "images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PE5IqhF7vX5b"
      },
      "cell_type": "markdown",
      "source": [
        "Showing 10 images in dataset"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "QDzYtiJ3vX5c"
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,20))\n",
        "for i in range(10, 20):\n",
        "    plt.subplot(6, 10, i-10+1)\n",
        "    cloth_img =  mpimg.imread(path + 'images/100'+ str(i) +'.jpg')\n",
        "    plt.imshow(cloth_img)\n",
        "    plt.axis(\"off\")\n",
        "plt.subplots_adjust(wspace=-0.5, hspace=1)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "75Aw0xINvX5c"
      },
      "cell_type": "markdown",
      "source": [
        "DataFrame with categories and adding column of image names"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "0eLe5-CsvX5d"
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(path + \"styles.csv\", nrows=6000, error_bad_lines=False)\n",
        "df['image'] = df.apply(lambda x: str(x['id']) + \".jpg\", axis=1)\n",
        "df = df.reset_index(drop=True)\n",
        "print(df.shape)\n",
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GChnu5cAvX5d"
      },
      "cell_type": "markdown",
      "source": [
        "Setting the Pre-Trained model DenseNet121"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "TGIRzh8QvX5d"
      },
      "cell_type": "code",
      "source": [
        "# Fine-tune DenseNet121 model\n",
        "densenet = DenseNet121(include_top=False, weights='imagenet', input_shape=(img_width, img_height, chnl))\n",
        "densenet.trainable = True  # Allow training on the DenseNet model\n",
        "\n",
        "# Add a global max pooling layer and additional dense layers for fine-tuning\n",
        "model = keras.Sequential([\n",
        "    densenet,\n",
        "    GlobalMaxPooling2D(),\n",
        "    keras.layers.Dense(256, activation='relu'),  # Add dense layer\n",
        "    keras.layers.Dropout(0.5),  # Add dropout for regularization\n",
        "    keras.layers.Dense(128, activation='relu')  # Final dense layer for feature extraction\n",
        "])\n",
        "\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model with a lower learning rate for fine-tuning\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-5), loss='categorical_crossentropy')\n",
        "\n",
        "# Assuming you have labels and a way to load images in batches for fine-tuning\n",
        "# For example, using ImageDataGenerator\n",
        "datagen = keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "\n",
        "train_directory = os.path.join(path, 'train')\n",
        "\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    directory=train_directory,  # Updated path to your training dataset\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "model.fit(train_generator, epochs=10)  # Adjust epochs as necessary"
      ],
      "metadata": {
        "id": "0FQAfRS1xEKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oMxKHHjYvX5e"
      },
      "cell_type": "markdown",
      "source": [
        "Function of model prediction"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "cqDQDEHJvX5e"
      },
      "cell_type": "code",
      "source": [
        "def img_path(img):\n",
        "    return path + 'images/' + img\n",
        "def model_predict(model, img_name):\n",
        "    # Reshape\n",
        "    img = image.load_img(img_path(img_name), target_size=(img_width, img_height))\n",
        "    # img to Array\n",
        "    x   = image.img_to_array(img)\n",
        "    # Expand Dim (1, w, h)\n",
        "    x   = np.expand_dims(x, axis=0)\n",
        "    # Pre process Input\n",
        "    x   = preprocess_input(x)\n",
        "    return model.predict(x).reshape(-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y4mvr-ewvX5e"
      },
      "cell_type": "markdown",
      "source": [
        "Building data frame of model prediction for all our images from dataset (getting embedding for all items in dataset)"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "4dIjdKy6vX5f"
      },
      "cell_type": "code",
      "source": [
        "df_copy      = df\n",
        "df_embedding = df_copy['image'].apply(lambda x: model_predict(model, x))\n",
        "df_embedding        = df_embedding.apply(pd.Series)\n",
        "df_embedding.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s2phMd4_vX5f"
      },
      "cell_type": "markdown",
      "source": [
        "Computing a cosine similarity to calculate a numeric quantity that denotes the similarity between two images. It is relatively easy and fast to calculate."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "jqR7UVw7vX5f"
      },
      "cell_type": "code",
      "source": [
        "cosine_sim = linear_kernel(df_embedding, df_embedding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DHU96N_yvX5f"
      },
      "cell_type": "markdown",
      "source": [
        "Getting indices"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "5VaMOfVNvX5f"
      },
      "cell_type": "code",
      "source": [
        "indices = pd.Series(range(len(df)), index=df.index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HOLIQwbnvX5f"
      },
      "cell_type": "markdown",
      "source": [
        "Getting recommendations using the cosine similarity"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "7oOQ5HwvvX5g"
      },
      "cell_type": "code",
      "source": [
        "def get_recommendations(index, df, cosine_sim=cosine_sim):\n",
        "    idx = indices[index]\n",
        "\n",
        "    # Get the pairwsie similarity scores of all clothes with that one\n",
        "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
        "\n",
        "    # Sort the clothes based on the similarity scores\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Get the scores of the 5 most similar clothes\n",
        "    sim_scores = sim_scores[1:6]\n",
        "\n",
        "    # Get the clothes indices\n",
        "    cloth_indices = [i[0] for i in sim_scores]\n",
        "\n",
        "    # Return the top 10 most similar movies\n",
        "    return df['image'].iloc[cloth_indices]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FjDVdGj4vX5g"
      },
      "cell_type": "markdown",
      "source": [
        "Checking the result"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "-ElWFVW3vX5g"
      },
      "cell_type": "code",
      "source": [
        "chosen_img_indx = 755\n",
        "\n",
        "recommendation = get_recommendations(chosen_img_indx, df, cosine_sim)\n",
        "recommendation_list = recommendation.to_list()\n",
        "#chosen image\n",
        "chosen_img =  mpimg.imread(path + 'images/' + df.iloc[chosen_img_indx].image)\n",
        "plt.title(\"Chosen image\")\n",
        "plt.imshow(chosen_img)\n",
        "#recommended images\n",
        "plt.figure(figsize=(20,20))\n",
        "j=0\n",
        "for i in recommendation_list:\n",
        "    plt.subplot(6, 10, j+1)\n",
        "    cloth_img =  mpimg.imread(path + 'images/'+ i)\n",
        "    plt.imshow(cloth_img)\n",
        "    plt.axis(\"off\")\n",
        "    j+=1\n",
        "plt.title(\"recommended images\")\n",
        "plt.subplots_adjust(wspace=-0.5, hspace=1)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "edlmNHYTvX5g"
      },
      "cell_type": "code",
      "source": [
        "chosen_img_indx = 2500\n",
        "\n",
        "recommendation = get_recommendations(chosen_img_indx, df, cosine_sim)\n",
        "recommendation_list = recommendation.to_list()\n",
        "#chosen image\n",
        "chosen_img =  mpimg.imread(path + 'images/' + df.iloc[chosen_img_indx].image)\n",
        "plt.title(\"Chosen image\")\n",
        "plt.imshow(chosen_img)\n",
        "#recommended images\n",
        "plt.figure(figsize=(20,20))\n",
        "j=0\n",
        "for i in recommendation_list:\n",
        "    plt.subplot(6, 10, j+1)\n",
        "    cloth_img =  mpimg.imread(path + 'images/'+ i)\n",
        "    plt.imshow(cloth_img)\n",
        "    plt.axis(\"off\")\n",
        "    j+=1\n",
        "plt.title(\"recommended images\")\n",
        "plt.subplots_adjust(wspace=-0.5, hspace=1)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "XFV3K5MhvX5g"
      },
      "cell_type": "code",
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Clothes Recommendation System Using DenseNet121",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}